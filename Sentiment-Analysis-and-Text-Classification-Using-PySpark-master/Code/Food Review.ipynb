{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis and Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Amazon Fine Food Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "### 1. Importing Libraries\n",
    "### 2. Creating SparkContext Object\n",
    " \n",
    "### 3. Preparing Data\n",
    "#### a. Understanding Data and removing unwanted columns\n",
    "####  b. Filtering neutral reviews\n",
    "####  c. Assigning Positive and Negative Sentiment to Reviews based on Score\n",
    "####  d. Assigning Binary Rating as Target Variable 1: Positive 0: Negative\n",
    " \n",
    "### 4. Text Pre-processing\n",
    "#### a. Create UDF Functions for text processing: Convert to lower case, Remove Punctuations and alphanumeric words, Remove Stop words\n",
    "#### b. POS tagging\n",
    "#### c. Text Lemmatization\n",
    "\n",
    "### 5. Preparing Data For Modelling\n",
    "#### a. Creating Final Dataset with apt columns\n",
    "#### b. Dividing it into Training and Test Set\n",
    "#### c. Tokenizing the Training set anc creating TF-IDF matrix using HashingTF\n",
    "\n",
    "\n",
    "### 6. Modelling the Data and Evaluating the Model\n",
    "#### a. Logistic Regression Model\n",
    "#### b. Naive Bayes Model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing findSPark to run Pyspark in Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark import HiveContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install vaderSentiment\n",
    "#Importing Library and setting environment path\n",
    "import os\n",
    "import sys\n",
    "#set the path \n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize   \n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SQLContext\n",
    "\n",
    "from pyspark import HiveContext\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes, GBTClassifier\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel,LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from vaderSentiment import vaderSentiment\n",
    "from pyspark.ml.feature import NGram\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating SparkContext Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "sqlContext=HiveContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### a. Understanding Data and removing unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 =sqlContext.read.format('com.databricks.spark.csv')\\\n",
    ".options(header='true',inferschema='true')\\\n",
    ".load(r\"C:\\Users\\singh\\OneDrive\\Documents\\Spring-19\\Big Data\\Project\\Food Review\\Food_Sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568454"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  b. Assigning Positive and Negative Sentiment to Reviews based on Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------+\n",
      "|Score|                Text|Sentiment|\n",
      "+-----+--------------------+---------+\n",
      "|    5|I have bought sev...| positive|\n",
      "|    1|\"Product arrived ...| negative|\n",
      "|    4|\"This is a confec...| positive|\n",
      "|    2|If you are lookin...| negative|\n",
      "+-----+--------------------+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def condition(r):\n",
    "    if (r <3):\n",
    "        label=\"negative\"\n",
    "    elif(r>3):\n",
    "        label=\"positive\"\n",
    "    else:\n",
    "        label=\"neutral\"\n",
    "    return label\n",
    "sentiment_udf = udf(lambda x: condition(x), StringType())\n",
    "\n",
    "df = df.withColumn('Sentiment',sentiment_udf(df['Score']))\n",
    "df.show(4,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  c. Filtering neutral reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525814"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "df=df.filter((f.col('Score')!=3))\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  d. Assigning Binary Rating as Target Variable 1: Positive 0: Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------+------+\n",
      "|Score|                Text|Sentiment|Target|\n",
      "+-----+--------------------+---------+------+\n",
      "|    5|I have bought sev...| positive|     1|\n",
      "|    1|\"Product arrived ...| negative|     0|\n",
      "|    4|\"This is a confec...| positive|     1|\n",
      "|    2|If you are lookin...| negative|     0|\n",
      "+-----+--------------------+---------+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def toBinary(score):\n",
    "    if score >= 3: return 1\n",
    "    else: return 0\n",
    "udfScoretoBinary=udf(toBinary, StringType())\n",
    "\n",
    "df = df.withColumn(\"Target\", udfScoretoBinary(\"Score\"))\n",
    "df.show(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Create UDF Functions for text processing: Convert to lower case, Remove Punctuations and alphanumeric words, Remove Stop words\n",
    "#### b. POS tagging\n",
    "#### c. Text Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT Pre-processing\n",
    "\n",
    "##COnvert to lower\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "lower_udf =udf(lower,StringType())\n",
    "\n",
    "\n",
    "##Remove nonAscii\n",
    "def strip_non_ascii(data_str):\n",
    "#''' Returns the string without non ASCII characters'''\n",
    "    stripped = (c for c in data_str if 0 < ord(c) < 127)\n",
    "    return ''.join(stripped)\n",
    "# setup pyspark udf function\n",
    "strip_non_ascii_udf = udf(strip_non_ascii, StringType())\n",
    "\n",
    "##FIx abbreviations\n",
    "def fix_abbreviation(data_str):\n",
    "    data_str = data_str.lower()\n",
    "    data_str = re.sub(r'\\bthats\\b', 'that is', data_str)\n",
    "    data_str = re.sub(r'\\bive\\b', 'i have', data_str)\n",
    "    data_str = re.sub(r'\\bim\\b', 'i am', data_str)\n",
    "    data_str = re.sub(r'\\bya\\b', 'yeah', data_str)\n",
    "    data_str = re.sub(r'\\bcant\\b', 'can not', data_str)\n",
    "    data_str = re.sub(r'\\bdont\\b', 'do not', data_str)\n",
    "    data_str = re.sub(r'\\bwont\\b', 'will not', data_str)\n",
    "    data_str = re.sub(r'\\bid\\b', 'i would', data_str)\n",
    "    data_str = re.sub(r'wtf', 'what the fuck', data_str)\n",
    "    data_str = re.sub(r'\\bwth\\b', 'what the hell', data_str)\n",
    "    data_str = re.sub(r'\\br\\b', 'are', data_str)\n",
    "    data_str = re.sub(r'\\bu\\b', 'you', data_str)\n",
    "    data_str = re.sub(r'\\bk\\b', 'OK', data_str)\n",
    "    data_str = re.sub(r'\\bsux\\b', 'sucks', data_str)\n",
    "    data_str = re.sub(r'\\bno+\\b', 'no', data_str)\n",
    "    data_str = re.sub(r'\\bcoo+\\b', 'cool', data_str)\n",
    "    data_str = re.sub(r'rt\\b', '', data_str)\n",
    "    data_str = data_str.strip()\n",
    "    return data_str\n",
    "\n",
    "\n",
    "##Remove punctuations mentions and alphanumeric characters\n",
    "def remove_features(data_str):\n",
    "# compile regex\n",
    "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n",
    "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    num_re = re.compile('(\\\\d+)')\n",
    "    mention_re = re.compile('@(\\w+)')\n",
    "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
    "# convert to lowercase\n",
    "    data_str = data_str.lower()\n",
    "# remove hyperlinks\n",
    "    data_str = url_re.sub(' ', data_str)\n",
    "# remove @mentions\n",
    "    data_str = mention_re.sub(' ', data_str)\n",
    "# remove puncuation\n",
    "    data_str = punc_re.sub(' ', data_str)\n",
    "# remove numeric 'words'\n",
    "    data_str = num_re.sub(' ', data_str)\n",
    "# remove non a-z 0-9 characters and words shorter than 1 characters\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    for word in data_str.split():\n",
    "        if list_pos == 0:\n",
    "            if alpha_num_re.match(word):\n",
    "                cleaned_str = word\n",
    "            else:\n",
    "                cleaned_str = ' '\n",
    "        else:\n",
    "            if alpha_num_re.match(word):\n",
    "                cleaned_str = cleaned_str + ' ' + word\n",
    "            else:\n",
    "                cleaned_str += ' '\n",
    "        list_pos += 1\n",
    "# remove unwanted space, *.split() will automatically split on\n",
    "# whitespace and discard duplicates, the \" \".join() joins the\n",
    "# resulting list into one string.\n",
    "    return \" \".join(cleaned_str.split())\n",
    "# setup pyspark udf function\n",
    "\n",
    "\n",
    "\n",
    "##Remove stop words\n",
    "def remove_stops(data_str):\n",
    "# expects a string\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    text = data_str.split()\n",
    "    for word in text:\n",
    "        if word not in stops:\n",
    "# rebuild cleaned_str\n",
    "            if list_pos == 0:\n",
    "                cleaned_str = word\n",
    "            else:\n",
    "                cleaned_str = cleaned_str + ' ' + word\n",
    "            list_pos += 1\n",
    "    return cleaned_str\n",
    "\n",
    "\n",
    "# Part-of-Speech Tagging\n",
    "def tag_and_remove(data_str):\n",
    "    cleaned_str = ' '\n",
    "# noun tags\n",
    "    nn_tags = ['NN', 'NNP', 'NNP', 'NNPS', 'NNS']\n",
    "# adjectives\n",
    "    jj_tags = ['JJ', 'JJR', 'JJS']\n",
    "# verbs\n",
    "    vb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    nltk_tags = nn_tags + jj_tags + vb_tags\n",
    "# break string into 'words'\n",
    "    text = data_str.split()\n",
    "# tag the text and keep only those with the right tags\n",
    "    tagged_text = pos_tag(text)\n",
    "    for tagged_word in tagged_text:\n",
    "        if tagged_word[1] in nltk_tags:\n",
    "            cleaned_str += tagged_word[0] + ' '\n",
    "    return cleaned_str\n",
    "\n",
    "\n",
    "##Lemmatization\n",
    "def lemmatize(data_str):\n",
    "# expects a string\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    text = data_str.split()\n",
    "    tagged_words = pos_tag(text)\n",
    "    for word in tagged_words:\n",
    "        if 'v' in word[1].lower():\n",
    "            lemma = lmtzr.lemmatize(word[0], pos='v')\n",
    "        else:\n",
    "            lemma = lmtzr.lemmatize(word[0], pos='n')\n",
    "        if list_pos == 0:\n",
    "            cleaned_str = lemma\n",
    "        else:\n",
    "            cleaned_str = cleaned_str + ' ' + lemma\n",
    "        list_pos += 1\n",
    "    return cleaned_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_udf =udf(lower,StringType())\n",
    "strip_non_ascii_udf = udf(strip_non_ascii, StringType())\n",
    "fix_abbreviation_udf = udf(fix_abbreviation, StringType())\n",
    "remove_features_udf = udf(remove_features, StringType())\n",
    "remove_stops_udf = udf(remove_stops, StringType())\n",
    "tag_and_remove_udf = udf(tag_and_remove, StringType())\n",
    "lemmatize_udf = udf(lemmatize, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"lower_text\",lower_udf(df[\"Text\"]))\n",
    "df = df.withColumn(\"text_non_asci\",fix_abbreviation_udf(df[\"lower_text\"]))\n",
    "df = df.withColumn(\"fixed_abbrev\",fix_abbreviation_udf(df[\"text_non_asci\"]))\n",
    "df = df.withColumn('removed_features',remove_features_udf(df['fixed_abbrev']))\n",
    "#df.show(5,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.withColumn('lemmatize_udf',remove_features_udf(df['tag_and_remove_udf']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|                Text|       removed_stops|Target|\n",
      "+--------------------+--------------------+------+\n",
      "|I have bought sev...|bought several vi...|     1|\n",
      "|\"Product arrived ...|product arrived l...|     0|\n",
      "|\"This is a confec...|confection around...|     1|\n",
      "|If you are lookin...|looking secret in...|     0|\n",
      "|Great taffy at a ...|great taffy great...|     1|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_no_stop_words = df.withColumn(\"removed_stops\", remove_stops_udf(\"removed_features\")).select('Text','removed_stops','Target')\n",
    "df_no_stop_words.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_tagging=df_no_stop_words.withColumn(\"tag_and_remove_pos\", tag_and_remove_udf(\"removed_stops\")).select('Text','tag_and_remove_pos','Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(words=['', 'bought', 'several', 'vitality', 'canned', 'dog', 'food', 'products', 'found', 'good', 'quality', 'product', 'looks', 'stew', 'processed', 'meat', 'smells', 'labrador', 'finicky', 'appreciates', 'product'], Target='1')\n",
      "Row(words=['', 'product', 'arrived', 'labeled', 'jumbo', 'salted', 'peanuts', 'peanuts', 'small', 'sized', 'unsalted', 'sure', 'error', 'vendor', 'intended', 'represent', 'product', 'jumbo'], Target='0')\n",
      "Row(words=['', 'confection', 'centuries', 'light', 'pillowy', 'citrus', 'gelatin', 'nuts', 'case', 'filberts', 'cut', 'tiny', 'squares', 'coated', 'powdered', 'sugar', 'tiny', 'mouthful', 'heaven', 'chewy', 'flavorful', 'recommend', 'yummy', 'treat', 'familiar', 'story', 'c', 'lion'], Target='1')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Tokenizing the document\n",
    "tokenizer = Tokenizer(inputCol=\"tag_and_remove_pos\", outputCol=\"words\")\n",
    "wordsDataFrame = tokenizer.transform(df_pos_tagging)\n",
    "for words_label in wordsDataFrame.select(\"words\", \"Target\").take(3):\n",
    "    print(words_label)\n",
    "\n",
    "df_text = df.withColumn(\"text_lower\",lower_udf(df[\"Text\"])).select('text_lower','Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|Target|      words_filtered|\n",
      "+------+--------------------+\n",
      "|     1|[, bought, severa...|\n",
      "|     0|[, product, arriv...|\n",
      "+------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"words_filtered\")\n",
    "wordsDataFrame1 = remover.transform(wordsDataFrame).select(\"Target\",\"words_filtered\")\n",
    "wordsDataFrame1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|                Text|     lemmatized_text|Target|\n",
      "+--------------------+--------------------+------+\n",
      "|I have bought sev...|buy several vital...|     1|\n",
      "|\"Product arrived ...|product arrive la...|     0|\n",
      "|\"This is a confec...|confection centur...|     1|\n",
      "|If you are lookin...|look secret ingre...|     0|\n",
      "|Great taffy at a ...|great taffy great...|     1|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_text_lemma=df_pos_tagging.withColumn(\"lemmatized_text\",lemmatize_udf(\"tag_and_remove_pos\")).select('Text','lemmatized_text','Target')\n",
    "df_text_lemma.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+---+\n",
      "|                Text|     lemmatized_text|Target|uid|\n",
      "+--------------------+--------------------+------+---+\n",
      "|I have bought sev...|buy several vital...|     1|  0|\n",
      "|\"Product arrived ...|product arrive la...|     0|  1|\n",
      "|\"This is a confec...|confection centur...|     1|  2|\n",
      "|If you are lookin...|look secret ingre...|     0|  3|\n",
      "+--------------------+--------------------+------+---+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "# Create Unique ID\n",
    "df_text_lemma = df_text_lemma.withColumn(\"uid\", monotonically_increasing_id())\n",
    "df_text_lemma.show(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Preparing Data For Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Creating Final Dataset with apt columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------+\n",
      "|uid|     lemmatized_text|Target|\n",
      "+---+--------------------+------+\n",
      "|  0|buy several vital...|     1|\n",
      "|  1|product arrive la...|     0|\n",
      "|  2|confection centur...|     1|\n",
      "|  3|look secret ingre...|     0|\n",
      "+---+--------------------+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = df_text_lemma.select('uid', 'lemmatized_text','Target')\n",
    "#data=wordsDataFrame2\n",
    "data.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_pd=data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_pd.to_csv(\"Sentiment_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[uid: bigint, lemmatized_text: string, Target: string]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Dividing it into Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching the RDD for training\n",
    "trainingData\n",
    "#Renaming features for modeling\n",
    "training = trainingData.selectExpr(\"lemmatized_text as text\", \"Target as label\")\n",
    "training = training.withColumn(\"label\", training[\"label\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching the RDD for test\n",
    "testData\n",
    "#Renaming features for modeling\n",
    "test = testData.selectExpr(\"lemmatized_text as text\", \"Target as label\")\n",
    "test = test.withColumn(\"label\", test[\"label\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Tokenizing the Training set anc creating TF-IDF matrix using HashingTF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"hashing\")\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Modelling and Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, lr])\n",
    "# Training the model\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicing Output\n",
    "prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|1.0  |1.0       |\n",
      "|1.0  |1.0       |\n",
      "|1.0  |1.0       |\n",
      "|1.0  |1.0       |\n",
      "|1.0  |1.0       |\n",
      "|1.0  |1.0       |\n",
      "|1.0  |1.0       |\n",
      "|1.0  |1.0       |\n",
      "|1.0  |1.0       |\n",
      "|1.0  |1.0       |\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.select(\"label\", \"prediction\").show(10,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- hashing: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.914613708621185"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "pipeline_nb = Pipeline(stages=[tokenizer, hashingTF, idf, nb])\n",
    "# Training the model\n",
    "model_nb = pipeline_nb.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicing Output\n",
    "prediction_nb = model_nb.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|1.0  |1.0       |\n",
      "|1.0  |1.0       |\n",
      "|1.0  |0.0       |\n",
      "|1.0  |0.0       |\n",
      "|1.0  |1.0       |\n",
      "|1.0  |1.0       |\n",
      "|1.0  |0.0       |\n",
      "|1.0  |1.0       |\n",
      "|1.0  |1.0       |\n",
      "|1.0  |1.0       |\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_nb.select(\"label\", \"prediction\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- hashing: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_nb.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8780527561465762"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator_nb = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator_nb.evaluate(prediction_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
